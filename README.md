# Creador de Embeddings local para ***B√∫squeda Sem√°ntica***
Crea embeddings usando el modelo de lenguaje natural ***paraphrase-multilingual-MiniLM-L12-v2***, usando un WebService REST como interfaz.

### En la ruta ***/process*** del WebService
Recibe un POST, esperando 2 par√°metros:
- mode: Que puede ser (single | chunks)
- textbase64: Ser√≠a el texto en Base64

Ejemplo:
```json
    {
        "mode": "single",
        "textbase64": "SG9sYSBNdW5kbyE="
    }
```


Cuando el **mode** es ***single***, el texto pasado en Base64, se limpia y se crea su embedding. Devolviendo la respuesta con el formato:
```json
    {
        "mode": "single",
        "text": Texto recibido y limpio,
        "embedding": arreglo de 128 n√∫meros de punto flotante.
    }
```

Cuando el **mode** es ***chunks***, el texto pasado en Base64, se limpia, se separa en chunks y a cada chunk se le crea su embedding. La respuesta devuelta tendr√≠a el formato siguiente:
```json
    {
        "mode": "chunks",
        "chunks": Arreglo chunks,
        "embedding": Arreglo de embeddings
    }
```

### En la ruta ***/test*** del WebService
Con una operaci√≥n GET, se recibe en el par√°metro **base64text**, un texto en *Base64*.
Retornando una salida con el siguiente formato: 
```json
    {
        "mode": "test",
        "text": Texto recibido y limpio,
        "embedding": arreglo de 128 n√∫meros de punto flotante.
    }
```
Ejemplo de uso: ```http://127.0.0.1:5000/test?base64text=SG9sYSBNdW5kbyE=```


### La ruta ***/health***, igualmente en ***/*** del WebService
Sirve para comprobar que el servicio esta trabajando.

---

## Modelo de Embeddings
Se utiliza el modelo ***paraphrase-multilingual-MiniLM-L12-v2***, que es de tama√±o medio y funcional para texto en varios idiomas (ingl√©s y espa√±ol incluidos). Con este modelo se generan vectores de 384 dimensiones.

Para evitar tener un esquema tan grande y previendo una indexaci√≥n de millones de embeddings, se han reducido a 128 dimensiones, usando una ***matriz PCA de reducci√≥n***, previamente entrenada, incluida en el proyecto.

### Acerca de la "Limpieza del texto" recibido
La limpieza del texto antes de trabajarlo, es indispensable para que el modelo de procesamiento de lenguaje natural **es_core_news_md**, cumpla adecuadamente su funci√≥n al separar en oraciones.

En este orden:
- Se quita todo el contenido delimitado por '***[[RS-***'  '***-RS]]***'.
- Se quitan todos los tokens : '***' (triple asterisco).
- Se quitan todos los tokens : '***[[¬°***'.
- Se reemplazan todos los tokens : '***!]]***' por '```. ```'.
- Se reemplazan todos los tokens : '***!]]***' por '```. ```'.
- Se reemplazan todos los tokens : ``` '<br /><br />'```' por '```\n```'.
- Se reemplazan todos los tokens : ``` '<br/><br/>'``` por '```\n```'.
- Se reemplazan todos los tokens : ``` '<br/>'``` por '```\n```'.
- Se reemplazan todos los tokens : ``` '<br />'``` por '```\n```'.
- Se eliminan el resto de TAGs de html.
- Se quitan todos los tokens : '***[[03]]***'.
- Se quitan todos los tokens : '***[[05]]***'.
- Normaliza diferentes tipos de comillas dobles y simples a s√≥lo tener las simples b√°sicas.
- Se reemplazan todos los tokens : '```\r\n\r\n```' por '```\r\n```'.


### L√≥gica de separaci√≥n de texto largo en varios Chunks
Una vez limpio el texto y aplicando el modelo de procesamiento de lenguaje natural **es_core_news_md** con Spacy; se separa en oraciones y aplicando  ***Similaridad de Coseno*** con un umbral de **0.6** de similitud, se agrupan oraciones consecutivas en un mismo ***chunk***.

En detalle, se obtienen embeddings para cada oraci√≥n, y usando operaciones de similitud entre esas oraciones consecutivas, se determinan si varias oraciones se pueden agrupar a un mismo chunk o se manejan en chunks separados. 

Tambi√©n se aplica la regla de que cada chunk tenga a al menos 150 caracteres. Con esto, oraciones muy cortas se agrupan en otras.

---

## Instalaci√≥n para Desarrollo
Si quieres tener la aplicaci√≥n funcionando en tu entorno de desarrollo:
 1. Clona el repositorio : ```git clone https://github.com/Hitokiri77777/Python-Embeddings-128.git```
 2. En tu terminal, c√°mbiate al folder creado.
 3. Crea el ambiente virtual con python. Usar : ``` python -m venv venv ```
 4. Activa el ambiente virtual:
    - En Windows : ``` venv\Scripts\activate ```
    - En Linux   : ``` source venv/bin/activate ```
 5. Instala las dependencias: ``` pip install -r requirements.txt ``` 
 6. Ejecuta la aplicaci√≥n : ``` python app.py ```

 * Podr√≠a usarse un servidor como "waitress" de Python, para mejorar respuestas de la aplicaci√≥n flask
   - Instala el paquete ```pip install waitress``` con el ambiente activado
   - Sirve la aplicaci√≥n de manera m√°s eficiente con el modelo multihilo de waitress con esto:

       ```waitress-serve --host=0.0.0.0 --port=5000 app:app```

- Ver prueba b√°sica de creaci√≥n de embedding en la √∫ltima secci√≥n del documento.
- Con la demostraci√≥n exitosa de la prueba; puedes ya hacer *POST* a la ruta **/process** para trabajar. 

---

## Creaci√≥n de Imagen para ***Docker***
Usar el archivo **Dockerfile** en la ra√≠z del proyecto, para creaci√≥n y puesta en marcha del contenedor. 

1. Teniendo Docker instalado. Hacer : ```docker build -t python_embeddings .```
   para crear la imagen.
2. Obten el ID de la imagen creada, listando las imagenes existentes con : ```docker images```
3. C√≥rrela, suponiendo que el ID es 562469e4e257 : ```docker run -p 5000:5000 562469e4e257```
   Con esto, el puerto 5000 de la imagen, se mapear√° al tambi√©n 5000 de tu m√°quina.
4. Ahora si podr√≠as hacer la prueba: ```http://127.0.0.1:5000/test?base64text=SG9sYSBNdW5kbyE=```

Se crear√≠a una imagen de ***2.1GB*** ya funcional.

***Nota Importante:*** Recuerda que la imagen de la aplicaci√≥n Flask, debe cargar el modelo *"paraphrase-multilingual-MiniLM-L12-v2"* desde internet en cada arranque. Ver el Log, para revisar cuando ya esta lista para trabajar.

Esto se puede evitar, descargando dicho modelo a un directorio, y hacer que forme parte de la imagen. Y modificar tambi√©n el c√≥digo fuente para cargarlo desde disco. Decid√≠ dejarlo de esa forma, para no tener que agregar ese directorio en el repositorio GIT. Son unos 480 MB.


### Sugerencia para llevarlo a Producci√≥n
Puedes poner esta aplicaci√≥n detr√°s de un servidor ***Nginx*** para mejorar rendimiento, seguridad y escalabilidad.

Para hacerlo, puedes usar el archivo ***docker-compose.yml***. Esto crear√≠a el flujo siguiente:
1. El usuario accede a http://localhost/.
2. Nginx recibe la solicitud en el puerto 80.
3. Nginx reenv√≠a la solicitud al contenedor de Flask en backend:5000.
4. Flask procesa la petici√≥n y devuelve la respuesta a Nginx.
5. Nginx env√≠a la respuesta final al usuario.

Utilizar√≠as el comando ```docker-compose up --build -d```
- Esto crear√≠a las 2 im√°genes, una para la App de Flask, nuestra app. Y otra con un linux Alpine s√∫per ligero, con un servidor de Nginx. Y un "puente de red" para hacer que las dos im√°genes trabajen en conjunto.

Tener una aplicaci√≥n Flask detr√°s de un servidor Nginx es una pr√°ctica recomendada en entornos de producci√≥n porque mejora la seguridad, el rendimiento y la escalabilidad.

### Problemas de ejecutar Flask directamente
Si se ejecuta Flask de forma nativa con python app.py nos enfrentar√≠amos a varias limitaciones:

- Flask no es eficiente manejando m√∫ltiples conexiones ‚Üí Puede procesar s√≥lo un n√∫mero limitado de solicitudes simult√°neamente.
- No maneja archivos est√°ticos eficientemente ‚Üí Sirviendo im√°genes, CSS o JS desde Flask, el rendimiento ser√° pobre (No es el caso de nuestra aplicaci√≥n).
- No soporta balanceo de carga ‚Üí No puedes escalar a m√∫ltiples instancias f√°cilmente.
- No maneja HTTPS nativamente ‚Üí No puedes configurar certificados SSL directamente en Flask.

### ¬øPor qu√© usar Nginx como proxy reverso?
Nginx es un servidor web ligero y eficiente que act√∫a como intermediario entre los clientes (navegador, API, etc.) y la aplicaci√≥n Flask.

- Mejora el rendimiento ‚Üí Puede manejar miles de conexiones concurrentes.
- Maneja archivos est√°ticos ‚Üí Como im√°genes, CSS y JS sin cargar Flask innecesariamente.
- Balanceo de carga ‚Üí Si usas m√∫ltiples instancias de Flask, distribuye las peticiones.
- Soporte SSL/TLS ‚Üí Se encarga de manejar certificados HTTPS.
- Protege contra ataques ‚Üí Como DDoS y accesos no autorizados.

### Beneficios en producci√≥n
- Optimizaci√≥n de tr√°fico ‚Üí Nginx maneja archivos est√°ticos, evitando que Flask se sobrecargue.
- Seguridad ‚Üí Evita que los clientes accedan directamente a Flask.
- Escalabilidad ‚Üí Puedes agregar m√∫ltiples instancias de Flask detr√°s de Nginx.
- HTTPS f√°cil ‚Üí Puedes configurar un certificado SSL en Nginx sin tocar Flask. 

### Si al final solo hay una aplicaci√≥n Flask ejecut√°ndose, ¬øpor qu√© Nginx mejora el rendimiento? 
La clave est√° en c√≥mo se manejan las conexiones y la distribuci√≥n de carga.

Flask maneja mal muchas conexiones concurrentes
Flask, por s√≠ solo, no est√° dise√±ado para manejar muchas conexiones al mismo tiempo, porque usa un servidor de desarrollo interno.

Nginx act√∫a como un buffer inteligente entre los clientes y Flask, mejorando el rendimiento por varios motivos:

1. Nginx maneja muchas conexiones simult√°neas eficientemente
Nginx usa un modelo as√≠ncrono basado en eventos, en lugar de crear un nuevo proceso/hilo para cada solicitud. Esto le permite gestionar miles de conexiones sin consumir muchos recursos.

üìå Ejemplo pr√°ctico:

Sin Nginx: Flask recibe 1000 peticiones y solo puede atender 5-10 a la vez. Las dem√°s quedan bloqueadas.

Con Nginx: Nginx recibe 1000 peticiones y distribuye las solicitudes a Flask de manera controlada. Mientras Flask procesa una, Nginx retiene las dem√°s sin bloquearlas.

2. Nginx maneja archivos est√°ticos sin pasar por Flask
Si sirves archivos como im√°genes, CSS o JS con Flask, cada solicitud consume recursos de Flask innecesariamente.
üöÄ Con Nginx, esos archivos se sirven directamente sin afectar el rendimiento de Flask.

üìå Ejemplo pr√°ctico:

Sin Nginx: Flask recibe 1000 solicitudes, incluyendo archivos est√°ticos. Se satura.

Con Nginx: Nginx atiende las solicitudes de archivos est√°ticos y solo pasa las solicitudes de API a Flask.

3. Nginx hace balanceo de carga (opcional)
Si tu aplicaci√≥n Flask crece, puedes correr m√∫ltiples instancias de Flask y usar Nginx para distribuir la carga.

üìå Ejemplo pr√°ctico:

Sin Nginx: Una sola instancia de Flask se satura con 1000 usuarios.

Con Nginx + 3 Flask: Nginx distribuye las solicitudes entre 3 instancias, mejorando la escalabilidad.

4. Nginx mantiene conexiones abiertas y usa cach√©
Cuando un cliente accede a Flask directamente, cada nueva conexi√≥n se abre y cierra.
üìå Con Nginx, las conexiones pueden mantenerse abiertas y reutilizarse, reduciendo la latencia.

üî• Conclusi√≥n:
Nginx no hace que Flask procese m√°s r√°pido, pero evita que Flask se sature y distribuye mejor las solicitudes. Por eso es fundamental para producci√≥n. 

***NOTA:*** Si vas a crear la imagen de Docker, que no sea desde el folder usado para una instalaci√≥n de desarrollo. S√≥lo agregar√≠as espacio innecesario a la imagen (incluir√≠a todo el subdirectorio /venv/).

## Creaci√≥n de Imagenes en Docker (Flask y Nginx)
Teniendo Docker instalado, c√°mbiate al directorio de la aplicaci√≥n. Y con el comando ```docker-compose up --build```
Eso crear√≠a 2 im√°genes en Docker dentro de una red.

Comprueba que las 2 im√°genes se estan ejecutando  con ```docker ps```

Comprueba que Nginx recibe las llamadas las direcciona a Flask:
```http://127.0.0.1/test?base64text=SG9sYSBNdW5kbyE=```
A diferencia de las pruebas anteriores, aqui no se indica el puerto 5000.

 ## Prueba b√°sica de creaci√≥n de Embedding
  - Usa: http://127.0.0.1:5000/test?base64text=SG9sYSBNdW5kbyE=
    √≥, cuando ya hay un servidor Nginx al frente: http://127.0.0.1/test?base64text=SG9sYSBNdW5kbyE=
  - Con esto env√≠as un texto corto en *Base64* sin usar *POST*.
  - Ver√°s el ***Embedding*** de resultado, con el texto recibido ya limpio.
  - Se usa *Base64*, porque el texto puede contener caracteres que pueden chocar con el esquema de una URL correcta o de un JSON bien formado. Aplica igual para el caso de los datos JSON recibidos en los *POST*.
  - Es posible que textos muy largos en *Base64*, sobrepasen el l√≠mite para *GET* de ***/test***, por si vas a probar textos propios en sea ruta **/test**.
